{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9462344,
          "sourceType": "datasetVersion",
          "datasetId": 5753003
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LSTM_reviews",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'reviews-shl:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5753003%2F9462344%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240926%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240926T225432Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dcafa1aacaac12bdb20c918acaaded3d261adf93760db0ba888e986aa2b35aa22cc930e12b946a11745d85a240e24ff0c1c57ec4333daf25c39738f3260c83f849d7fab15f39bef76034e1a6a2c6f70d2b5fac8ae4e6b968b2c89884b500197140ba17c0b8e23da51b2e72b79c6705e9d78004cd2dfdf104ff0cc51cd613fa8ec191101a9b913b318ce2891e4ec0f9817c4fbbfd5c6321ff12065f67afe2f79cdb93a2356c08d6b9980d9f0d89f22a42b243f9ff0561ccba4712c9c403f8d95ee47b1cec94a9fd403086902e9ec40debb130f143c65711090ba83534b8954b45aea60345d95c4098bb841dfa1d1d571dc4fa580af3d36b41f727b7fbc3e010499'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jaOqLve4iy2p"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:06.899233Z",
          "iopub.execute_input": "2024-09-25T07:20:06.899859Z",
          "iopub.status.idle": "2024-09-25T07:20:06.906338Z",
          "shell.execute_reply.started": "2024-09-25T07:20:06.899819Z",
          "shell.execute_reply": "2024-09-25T07:20:06.905335Z"
        },
        "trusted": true,
        "id": "WB_LCx2aiy2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:07.658959Z",
          "iopub.execute_input": "2024-09-25T07:20:07.659606Z",
          "iopub.status.idle": "2024-09-25T07:20:09.468401Z",
          "shell.execute_reply.started": "2024-09-25T07:20:07.659565Z",
          "shell.execute_reply": "2024-09-25T07:20:09.466962Z"
        },
        "trusted": true,
        "id": "CYxZaScWiy2w",
        "outputId": "45e28e31-3b85-42b2-819f-3d7bf31e7950"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Attempt to download the WordNet corpus\n",
        "nltk.download('wordnet', download_dir='/usr/share/nltk_data')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:09.470788Z",
          "iopub.execute_input": "2024-09-25T07:20:09.47196Z",
          "iopub.status.idle": "2024-09-25T07:20:09.480186Z",
          "shell.execute_reply.started": "2024-09-25T07:20:09.471909Z",
          "shell.execute_reply": "2024-09-25T07:20:09.479122Z"
        },
        "trusted": true,
        "id": "MzJZw0S1iy2z",
        "outputId": "435e2cea-7b26-43f4-97c9-7116e03f34ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the CSV file directly\n",
        "article_df = pd.read_csv('/kaggle/input/reviews-shl/reviews_supplements.csv')\n",
        "\n",
        "# Extract the 'text' column data and store it in a list, dropping NaN values\n",
        "all_texts = article_df['text'].dropna().values  # Removes NaN (missing) values\n",
        "\n",
        "# Remove rows with 'Unknown' values\n",
        "all_texts = [t for t in all_texts if t != \"Unknown\"]\n",
        "\n",
        "# Function to clean text using spaCy\n",
        "def clean_text(txt):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(txt)\n",
        "    # Remove punctuation and stop words, and lemmatize\n",
        "    cleaned_words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(cleaned_words)  # Return cleaned text as a single string\n",
        "\n",
        "# Apply the clean_text function to each entry in all_texts\n",
        "corpus = [clean_text(x) for x in all_texts]\n",
        "\n",
        "# View the first 10 cleaned texts\n",
        "print(corpus[:10])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:21:48.669118Z",
          "iopub.execute_input": "2024-09-25T07:21:48.669835Z",
          "iopub.status.idle": "2024-09-25T07:21:49.578142Z",
          "shell.execute_reply.started": "2024-09-25T07:21:48.669796Z",
          "shell.execute_reply": "2024-09-25T07:21:49.576591Z"
        },
        "trusted": true,
        "id": "0hcFLENtiy20",
        "outputId": "3901e757-458a-4c9a-9124-cf86c1736023"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the spaCy English model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/compat.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/thinc/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/thinc/config.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/thinc/types.py:25\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Any,\n\u001b[1;32m      6\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     overload,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/thinc/compat.py:35\u001b[0m\n\u001b[1;32m     31\u001b[0m     has_cupy_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     has_torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:764\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name '_C' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Initialize the Tokenizer with a limited vocabulary size\n",
        "max_words = 8000  # Adjust this value based on your needs\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Convert data to sequence of tokens\n",
        "max_sequence_len = 150  # Adjust this value based on your needs\n",
        "\n",
        "def generate_sequences(texts):\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            if len(n_gram_sequence) > max_sequence_len:\n",
        "                n_gram_sequence = n_gram_sequence[-max_sequence_len:]\n",
        "            sequences.append(n_gram_sequence)\n",
        "    return sequences\n",
        "\n",
        "# Generate sequences\n",
        "input_sequences = generate_sequences(corpus)\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Separate predictors and label\n",
        "predictors, label = padded_sequences[:,:-1], padded_sequences[:,-1]\n",
        "\n",
        "# One-hot encode labels\n",
        "label = to_categorical(label, num_classes=max_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:17.955751Z",
          "iopub.status.idle": "2024-09-25T07:20:17.956097Z",
          "shell.execute_reply.started": "2024-09-25T07:20:17.955929Z",
          "shell.execute_reply": "2024-09-25T07:20:17.955946Z"
        },
        "trusted": true,
        "id": "hd8wY1-Kiy22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After generating sequences and padding\n",
        "print(\"Predictors shape:\", predictors.shape)\n",
        "print(\"Label shape:\", label.shape)\n",
        "\n",
        "# Update these variables based on the actual shapes\n",
        "max_sequence_len = predictors.shape[1] + 1  # +1 because predictors don't include the last word\n",
        "max_words = label.shape[1]\n",
        "\n",
        "# Create the model\n",
        "def create_model(max_words, max_sequence_len):\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 10, input_length=max_sequence_len-1),\n",
        "        LSTM(50, return_sequences=False),\n",
        "        Dropout(0.1),\n",
        "        Dense(max_words, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model(max_words, max_sequence_len)\n",
        "\n",
        "# Build the model with the correct input shape\n",
        "model.build(input_shape=(None, max_sequence_len-1))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:17.957793Z",
          "iopub.status.idle": "2024-09-25T07:20:17.958258Z",
          "shell.execute_reply.started": "2024-09-25T07:20:17.958013Z",
          "shell.execute_reply": "2024-09-25T07:20:17.958038Z"
        },
        "trusted": true,
        "id": "h_ZfDfrRiy22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(total_words, max_sequence_len=300):\n",
        "    input_len = max_sequence_len - 1\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=total_words, output_dim=10, input_length=input_len))\n",
        "    model.add(LSTM(50, return_sequences=False))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with explicit input shape\n",
        "input_len = predictors.shape[1]\n",
        "model = create_model(total_words, max_sequence_len=input_len+1)\n",
        "\n",
        "# Build the model with sample input\n",
        "model.build(input_shape=(None, input_len))\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:20:19.954051Z",
          "iopub.execute_input": "2024-09-25T07:20:19.954336Z",
          "iopub.status.idle": "2024-09-25T07:20:19.993593Z",
          "shell.execute_reply.started": "2024-09-25T07:20:19.954306Z",
          "shell.execute_reply": "2024-09-25T07:20:19.992411Z"
        },
        "trusted": true,
        "id": "edI2yzE9iy23",
        "outputId": "7044892b-6331-4b33-d20b-786473b9b739"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create the model with explicit input shape\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m input_len \u001b[38;5;241m=\u001b[39m \u001b[43mpredictors\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(total_words, max_sequence_len\u001b[38;5;241m=\u001b[39minput_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Build the model with sample input\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predictors' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'predictors' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Bidirectional\n",
        "\n",
        "def create_model(max_words, max_sequence_len):\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 100, input_length=max_sequence_len-1),  # Increase embedding dimension\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),  # First Bidirectional LSTM layer\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(128)),  # Second Bidirectional LSTM layer\n",
        "        Dropout(0.3),\n",
        "        Dense(max_words, activation='softmax')  # Softmax for multi-class classification\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "k9WXvCTLiy25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "batch_size = 26  # Adjust this value based on your memory constraints\n",
        "epochs = 50  # You may want to increase this for better results\n",
        "\n",
        "history = model.fit(predictors, label, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(\"Model training completed.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T20:31:56.508688Z",
          "iopub.execute_input": "2024-09-23T20:31:56.509477Z",
          "iopub.status.idle": "2024-09-23T21:23:51.704528Z",
          "shell.execute_reply.started": "2024-09-23T20:31:56.509432Z",
          "shell.execute_reply": "2024-09-23T21:23:51.703371Z"
        },
        "trusted": true,
        "id": "HPO-sSPiiy26",
        "outputId": "6fa7b142-cada-4c5f-a3b7-2f55de7eda7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.8652\nEpoch 2/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.8255\nEpoch 3/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.7979\nEpoch 4/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.7761\nEpoch 5/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.7465\nEpoch 6/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.7334\nEpoch 7/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.7022\nEpoch 8/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.6885\nEpoch 9/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.6650\nEpoch 10/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.6494\nEpoch 11/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.6435\nEpoch 12/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.6206\nEpoch 13/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.6188\nEpoch 14/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5939\nEpoch 15/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5925\nEpoch 16/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.5727\nEpoch 17/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5589\nEpoch 18/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.5556\nEpoch 19/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5553\nEpoch 20/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5378\nEpoch 21/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.5332\nEpoch 22/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5149\nEpoch 23/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5180\nEpoch 24/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.5069\nEpoch 25/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4990\nEpoch 26/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4855\nEpoch 27/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4811\nEpoch 28/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.4778\nEpoch 29/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.4678\nEpoch 30/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.4640\nEpoch 31/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.4590\nEpoch 32/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4508\nEpoch 33/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4374\nEpoch 34/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4450\nEpoch 35/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 8ms/step - loss: 5.4341\nEpoch 36/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.4413\nEpoch 37/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 8ms/step - loss: 5.4210\nEpoch 38/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.4179\nEpoch 39/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.4236\nEpoch 40/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.4097\nEpoch 41/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7ms/step - loss: 5.4014\nEpoch 42/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.4029\nEpoch 43/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3996\nEpoch 44/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3910\nEpoch 45/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3844\nEpoch 46/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3870\nEpoch 47/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3749\nEpoch 48/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - loss: 5.3830\nEpoch 49/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 7ms/step - loss: 5.3759\nEpoch 50/50\n\u001b[1m8242/8242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 7ms/step - loss: 5.3674\nModel training completed.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, next_words, max_sequence_len, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # Pad the sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predicted = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        predicted = np.log(predicted + 1e-10) / temperature\n",
        "        predicted = np.exp(predicted) / np.sum(np.exp(predicted))\n",
        "\n",
        "        # Sample the next word\n",
        "        predicted_word_index = np.random.choice(range(len(predicted)), p=predicted)\n",
        "\n",
        "        # Get the actual word from the index\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage with temperature\n",
        "generated_text = generate_text(model, tokenizer, seed_text, next_words=20, max_sequence_len=max_sequence_len, temperature=0.8)\n",
        "print(generated_text)\n",
        "\n",
        "# Example usage with different temperatures\n",
        "generated_text_low_temp = generate_text(model, tokenizer, seed_text, next_words=20, max_sequence_len=max_sequence_len, temperature=0.5)  # Less random\n",
        "print(\"Low Temperature Output:\", generated_text_low_temp)\n",
        "\n",
        "generated_text_high_temp = generate_text(model, tokenizer, seed_text, next_words=20, max_sequence_len=max_sequence_len, temperature=1.5)  # More random\n",
        "print(\"High Temperature Output:\", generated_text_high_temp)\n",
        "\n",
        "# Example usage with different temperatures\n",
        "generated_text_low_temp = generate_text(model, tokenizer, seed_text, next_words=20, max_sequence_len=max_sequence_len, temperature=0.5)  # Less random\n",
        "print(\"Low Temperature Output:\", generated_text_low_temp)\n",
        "\n",
        "generated_text_high_temp = generate_text(model, tokenizer, seed_text, next_words=20, max_sequence_len=max_sequence_len, temperature=1)  # More random\n",
        "print(\"High Temperature Output:\", generated_text_high_temp)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:35:12.572992Z",
          "iopub.execute_input": "2024-09-23T21:35:12.573427Z",
          "iopub.status.idle": "2024-09-23T21:35:18.947724Z",
          "shell.execute_reply.started": "2024-09-23T21:35:12.573388Z",
          "shell.execute_reply": "2024-09-23T21:35:18.946453Z"
        },
        "trusted": true,
        "id": "Csft_rzLiy27",
        "outputId": "5801a744-e5d6-46bc-bea6-3014522dc71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "This product is fast read low check new thermometer br time find guy honest accuracy flexible tip little large light big use 100\nLow Temperature Output: This product is work work great price good product good price great price product reorder product good price price great value try taste\nHigh Temperature Output: This product is tough rate size fragile notice outside temp loss hormone drop state important 5 mind grow need take necessary exposure try\nLow Temperature Output: This product is work work work month stop work week ago get week get 3 week use disappointed seller receive refund new product\nHigh Temperature Output: This product is get dr month disappointed work describe friend start lemon time feel day need daily daily result kick need thank process\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the function to generate text with temperature scaling\n",
        "def generate_text(model, tokenizer, seed_text, next_words, max_sequence_len, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # Pad the sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predicted = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        predicted = np.log(predicted + 1e-10) / temperature\n",
        "        predicted = np.exp(predicted) / np.sum(np.exp(predicted))\n",
        "\n",
        "        # Sample the next word\n",
        "        predicted_word_index = np.random.choice(range(len(predicted)), p=predicted)\n",
        "\n",
        "        # Get the actual word from the index\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate multiple synthetic reviews\n",
        "def generate_multiple_reviews(model, tokenizer, num_reviews=500):\n",
        "    reviews = []\n",
        "\n",
        "    # Define seed phrases for different sentiments\n",
        "    seed_phrases = {\n",
        "        \"good\": [\"Great product\", \"I love this supplement\", \"Highly effective\", \"Fantastic results\", \"Worth every penny\"],\n",
        "        \"bad\": [\"Did not work for me\", \"Waste of money\", \"Not worth it\", \"Terrible experience\", \"Would not recommend\"],\n",
        "        \"neutral\": [\"It's okay\", \"Average supplement\", \"Nothing special\", \"Mediocre product\", \"Satisfactory\"],\n",
        "    }\n",
        "\n",
        "    # Flatten the seed phrases into a list for random selection\n",
        "    all_seeds = [phrase for sentiments in seed_phrases.values() for phrase in sentiments]\n",
        "\n",
        "    for i in range(num_reviews):\n",
        "        # Randomly choose a seed from the available phrases\n",
        "        seed_text = np.random.choice(all_seeds)\n",
        "\n",
        "        # Set random temperature between 0.5 and 1.5\n",
        "        temperature = np.random.uniform(0.5, 1.5)\n",
        "\n",
        "        # Set random length for the generated review between 5 and 10 words\n",
        "        next_words = np.random.randint(5, 11)\n",
        "\n",
        "        # Generate text using the generate_text function\n",
        "        generated_text = generate_text(model, tokenizer, seed_text, next_words=next_words, max_sequence_len=max_sequence_len, temperature=temperature)\n",
        "\n",
        "        # Append the generated review to the list\n",
        "        reviews.append(generated_text)\n",
        "\n",
        "        if (i + 1) % 50 == 0:  # Print status every 50 reviews\n",
        "            print(f\"Generated {i + 1} reviews so far...\")\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Generate reviews\n",
        "generated_reviews = generate_multiple_reviews(model, tokenizer)\n",
        "\n",
        "# Save generated reviews to a DataFrame\n",
        "reviews_df = pd.DataFrame(generated_reviews, columns=[\"Review\"])\n",
        "\n",
        "# Save to CSV\n",
        "reviews_df.to_csv(\"synthetic_amazon_reviews.csv\", index=False)\n",
        "\n",
        "print(\"500 synthetic reviews generated and saved to 'synthetic_amazon_reviews.csv'.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:45:05.386415Z",
          "iopub.execute_input": "2024-09-23T21:45:05.38712Z",
          "iopub.status.idle": "2024-09-23T21:49:11.420943Z",
          "shell.execute_reply.started": "2024-09-23T21:45:05.387072Z",
          "shell.execute_reply": "2024-09-23T21:49:11.419917Z"
        },
        "trusted": true,
        "id": "PIIB8fxGiy28",
        "outputId": "510b0bed-c0cc-4dd5-c327-513b30061923"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated 50 reviews so far...\nGenerated 100 reviews so far...\nGenerated 150 reviews so far...\nGenerated 200 reviews so far...\nGenerated 250 reviews so far...\nGenerated 300 reviews so far...\nGenerated 350 reviews so far...\nGenerated 400 reviews so far...\nGenerated 450 reviews so far...\nGenerated 500 reviews so far...\n500 synthetic reviews generated and saved to 'synthetic_amazon_reviews.csv'.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the function to generate text with temperature scaling\n",
        "def generate_text(model, tokenizer, seed_text, next_words, max_sequence_len, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # Pad the sequence\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        # Predict the next word\n",
        "        predicted = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        predicted = np.log(predicted + 1e-10) / temperature\n",
        "        predicted = np.exp(predicted) / np.sum(np.exp(predicted))\n",
        "\n",
        "        # Sample the next word\n",
        "        predicted_word_index = np.random.choice(range(len(predicted)), p=predicted)\n",
        "\n",
        "        # Get the actual word from the index\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Function to generate multiple synthetic reviews\n",
        "def generate_multiple_reviews(model, tokenizer, num_reviews=200):  # Set to 200 reviews\n",
        "    reviews = []\n",
        "\n",
        "    # Define seed phrases for different sentiments\n",
        "    seed_phrases = {\n",
        "        \"good\": [\"Great product\", \"I love this supplement\", \"Highly effective\", \"Fantastic results\", \"Worth every penny\"],\n",
        "        \"bad\": [\"Did not work for me\", \"Waste of money\", \"Not worth it\", \"Terrible experience\", \"Would not recommend\"],\n",
        "        \"neutral\": [\"It's okay\", \"Average supplement\", \"Nothing special\", \"Mediocre product\", \"Satisfactory\"],\n",
        "    }\n",
        "\n",
        "    # Flatten the seed phrases into a list for random selection\n",
        "    all_seeds = [phrase for sentiments in seed_phrases.values() for phrase in sentiments]\n",
        "\n",
        "    for i in range(num_reviews):\n",
        "        # Randomly choose a seed from the available phrases\n",
        "        seed_text = np.random.choice(all_seeds)\n",
        "\n",
        "        # Set random temperature between 0.5 and 1.5\n",
        "        temperature = np.random.uniform(0.5, 1.5)\n",
        "\n",
        "        # Set random length for the generated review between 5 and 10 words\n",
        "        next_words = np.random.randint(5, 11)\n",
        "\n",
        "        # Generate text using the generate_text function\n",
        "        generated_text = generate_text(model, tokenizer, seed_text, next_words=next_words, max_sequence_len=max_sequence_len, temperature=temperature)\n",
        "\n",
        "        # Append the generated review to the list\n",
        "        reviews.append(generated_text)\n",
        "\n",
        "        if (i + 1) % 50 == 0:  # Print status every 50 reviews\n",
        "            print(f\"Generated {i + 1} reviews so far...\")\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Generate reviews\n",
        "generated_reviews = generate_multiple_reviews(model, tokenizer)\n",
        "\n",
        "# Save generated reviews to a DataFrame\n",
        "reviews_df = pd.DataFrame(generated_reviews, columns=[\"Review\"])\n",
        "\n",
        "# Save to CSV file in Kaggle environment\n",
        "output_file_path = \"/kaggle/working/synthetic_amazon_reviews.csv\"\n",
        "reviews_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"200 synthetic reviews generated and saved to '{output_file_path}'.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:51:33.021964Z",
          "iopub.execute_input": "2024-09-23T21:51:33.022445Z",
          "iopub.status.idle": "2024-09-23T21:53:12.239289Z",
          "shell.execute_reply.started": "2024-09-23T21:51:33.022403Z",
          "shell.execute_reply": "2024-09-23T21:53:12.238059Z"
        },
        "trusted": true,
        "id": "8jtEFTYTiy29",
        "outputId": "7cf49678-d7ad-40d5-a4f2-2add5f6836af"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated 50 reviews so far...\nGenerated 100 reviews so far...\nGenerated 150 reviews so far...\nGenerated 200 reviews so far...\n200 synthetic reviews generated and saved to '/kaggle/working/synthetic_amazon_reviews.csv'.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in working directory:\", os.listdir(\"/kaggle/working/\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:54:58.794592Z",
          "iopub.execute_input": "2024-09-23T21:54:58.79506Z",
          "iopub.status.idle": "2024-09-23T21:54:58.801136Z",
          "shell.execute_reply.started": "2024-09-23T21:54:58.795022Z",
          "shell.execute_reply": "2024-09-23T21:54:58.800219Z"
        },
        "trusted": true,
        "id": "fbbwOecjiy2_",
        "outputId": "21ce3ef6-175e-49c3-e113-6b0f3afd7a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files in working directory: ['synthetic_amazon_reviews.csv', '.virtual_documents']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the generated CSV file\n",
        "reviews_df = pd.read_csv('/kaggle/working/synthetic_amazon_reviews.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(reviews_df.iloc[132])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-23T21:59:36.404051Z",
          "iopub.execute_input": "2024-09-23T21:59:36.404502Z",
          "iopub.status.idle": "2024-09-23T21:59:36.413039Z",
          "shell.execute_reply.started": "2024-09-23T21:59:36.404458Z",
          "shell.execute_reply": "2024-09-23T21:59:36.412068Z"
        },
        "trusted": true,
        "id": "hcOmmW5viy2_",
        "outputId": "15b6e511-cdf9-4976-82ee-654645b0fc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Review    Terrible experience take vitamin try good tast...\nName: 132, dtype: object\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the generated CSV file\n",
        "reviews_df = pd.read_csv('/kaggle/working/synthetic_amazon_reviews.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(reviews_df.iloc[133])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-25T07:16:48.60324Z",
          "iopub.execute_input": "2024-09-25T07:16:48.604022Z",
          "iopub.status.idle": "2024-09-25T07:16:49.778454Z",
          "shell.execute_reply.started": "2024-09-25T07:16:48.603987Z",
          "shell.execute_reply": "2024-09-25T07:16:49.776728Z"
        },
        "trusted": true,
        "id": "LXWIIZuViy3A",
        "outputId": "49c87bf7-11ff-4be3-de81-1b96946e0118"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the generated CSV file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/synthetic_amazon_reviews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the DataFrame\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(reviews_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m133\u001b[39m])\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/synthetic_amazon_reviews.csv'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/working/synthetic_amazon_reviews.csv'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYClaT7Uiy3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}